# Slurm Job Array to run FreeSurfer on HPC clusters using local parallelization on the nodes via GNU Parallel

These scripts split a subjects file into several chunks and provide a suitable [Slurm](https://slurm.schedmd.com) job script to run FreeSurfer for many subjects on high performance computing (HPC) systems. The submit script makes use of Slurm job arrays.

You can use the provided job submission script (submit.sh) as a rough template, but you will of course have to adapt it to the setup of cluster and its file systems, rules and resource limits.

The files in this directory can also solve the special case where you want to run local parallelization of several FreeSurfer jobs on the cores of a single worker of a HPC system. This approach can dramatically speed up your computations if the cluster assigns full nodes to a single job.


## Usage scenario

You want to run FreeSurfer for many subjects on a HPC cluster.

You already have or can obtain:

* a FreeSurfer installation on the cluster (i.e., accessible from the worker nodes)
* a GNU parallel installation on the cluster
* your MRI data, available on the cluster, includings a `subjects.txt` file for FreeSurfer
* if the cluster imposes a job limit, you need to know it

## What this folder provides

* The `split_subjects_txt.py` script splits your `subjects.txt` file into chunks (files named `subjects_jobX.txt`, where `X` is a running index). If there is a job limit on the cluster, use that number as the number of splits. If there is none, the number of chunks will be set to the number of subjects.
* The `submit.sh` script is designed to be submitted to the scheduler with the `sbatch` command from slurm. Each job in the job array started by the submission will use one of the split `subjects_jobX.txt` files generated by the `split_subjects_txt.py` script.