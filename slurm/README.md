# Slurm Job Array to run FreeSurfer on HPC clusters using local parallelization on the nodes via GNU Parallel

These scripts split a subjects file into several chunks and provide a suitable [Slurm](https://slurm.schedmd.com) job script to run FreeSurfer for many subjects on high performance computing (HPC) systems. The submit script makes use of Slurm job arrays.

You can use the provided job submission script (submit.sh) as a rough template, but you will of course have to adapt it to the setup of cluster and its file systems, rules and resource limits.

The files in this directory can also solve the special case where you want to run local parallelization of several FreeSurfer jobs on the cores of a single worker of a HPC system. This approach can dramatically speed up your computations if the cluster assigns full nodes to a single job.


## Usage scenario

You want to run FreeSurfer for many subjects on a HPC cluster.

You already have or can obtain:

* a FreeSurfer installation on the cluster (i.e., accessible from the worker nodes)
* a GNU parallel installation on the cluster
* your MRI data, available on the cluster, includings a `subjects.txt` file for FreeSurfer
* if the cluster imposes a job limit, you need to know it

## What this folder provides

* The `split_subjects_txt.py` script splits your `subjects.txt` file into chunks (files named `subjects_jobX.txt`, where `X` is a running index). If there is a job limit on the cluster, use that number as the number of splits. If there is none, the number of chunks will be set to the number of subjects.
* The `submit.sh` script is designed to be submitted to the scheduler with the `sbatch` command from slurm. Each job in the job array started by the submission will use one of the split `subjects_jobX.txt` files generated by the `split_subjects_txt.py` script.
* The `subjects.txt` file is just a demo file. Replace it with your own.


## Installation

1. Clone this repo to your home on the cluster. Log in to the cluster, then: ```git clone https://github.com/dfsp-spirit/freesurfer_parallel_scripts```.
2. Change into the cloned directory: ```cd freesurfer_parallel_scripts/slurm/```


## Usage

1. Replace the demo `subjects.txt` file in the directory with your own file.
2. Split your `subjects.txt` file with the provided script. Let's assume you want 20 jobs, then you would do: ```python split_subjects_txt.py 20 subjects.txt```
3. Adapt the `submit.sh` script to the environment of your HPC cluster. E.g., adapt the path to your MRI data on the cluster in the scripts, and any other variables that require a change. Read the cluster-specific documentation or ask your cluster admin team if you need help. Please do not contact me, I cannot help, as the setup you need is specific to your cluster.
4. Run ```sbatch submit.sh``` on the cluster head to submit the jobs.

